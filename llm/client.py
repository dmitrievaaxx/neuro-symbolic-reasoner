import os
from typing import Callable, Awaitable, Any
from functools import lru_cache

from dotenv import load_dotenv
from openai import AsyncOpenAI, APIError

from llm.resolver import resolution_proof


load_dotenv()


MODELS = [
    "meta-llama/llama-3.3-70b-instruct:free",
    "deepseek/deepseek-r1-0528-qwen3-8b:free",
]


# Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð° Ð´Ð»Ñ ÑƒÐºÐ°Ð·Ð°Ð½Ð½Ð¾Ð³Ð¾ Ð¼Ð¾Ð´ÑƒÐ»Ñ (formalizer, explainer)
@lru_cache(maxsize=3)
def _get_prompt(module: str) -> str:
    prompts_dir = os.path.join(os.path.dirname(__file__), "prompts")
    prompt_path = os.path.join(prompts_dir, f"{module}.txt")
    try:
        with open(prompt_path, "r", encoding="utf-8") as f:
            return f.read().strip()
    except FileNotFoundError:
        raise RuntimeError(f"ÐŸÑ€Ð¾Ð¼Ð¿Ñ‚ Ð´Ð»Ñ Ð¼Ð¾Ð´ÑƒÐ»Ñ '{module}' Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½: {prompt_path}")


# Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° ÑÐ¸ÑÑ‚ÐµÐ¼Ð½Ð¾Ð³Ð¾ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð° (legacy, Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾Ð¹ ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼Ð¾ÑÑ‚Ð¸)
@lru_cache(maxsize=1)
def _get_system_prompt() -> str:
    prompt_path = os.path.join(os.path.dirname(__file__), "system_prompt.txt")
    try:
        with open(prompt_path, "r", encoding="utf-8") as f:
            return f.read().strip()
    except FileNotFoundError:
        return (
            "You are a helpful assistant answering in Russian by default. "
            "Give concise and clear answers."
        )


# Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð° OpenAI Ð´Ð»Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ OpenRouter
@lru_cache(maxsize=1)
def _get_client() -> AsyncOpenAI:
    api_key = os.getenv("OPENROUTER_API_KEY")
    if not api_key:
        raise RuntimeError("OPENROUTER_API_KEY is not set in environment")

    return AsyncOpenAI(
        api_key=api_key,
        base_url="https://openrouter.ai/api/v1",
    )


# Ð’Ñ‹Ð·Ð¾Ð² LLM Ñ Ð¼ÐµÑ…Ð°Ð½Ð¸Ð·Ð¼Ð¾Ð¼ fallback (ÐµÑÐ»Ð¸ Ð¾Ð´Ð½Ð° Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð½Ðµ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚, Ð¿Ñ€Ð¾Ð±ÑƒÐµÑ‚ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰ÑƒÑŽ)
async def _call_llm(system_prompt: str, user_text: str, user_id: str | None = None) -> str:
    client = _get_client()

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_text},
    ]

    extra_headers = {}
    if user_id:
        extra_headers["X-Title"] = f"tg-user-{user_id}"

    last_error = None

    # ÐŸÑ€Ð¾Ð±ÑƒÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¿Ð¾ Ð¾Ñ‡ÐµÑ€ÐµÐ´Ð¸ Ð´Ð¾ Ð¿ÐµÑ€Ð²Ð¾Ð¹ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾Ð¹
    for model in MODELS:
        try:
            response = await client.chat.completions.create(
                model=model,
                messages=messages,
                extra_headers=extra_headers or None,
            )

            choice = response.choices[0]
            content = choice.message.content or ""
            return content.strip()

        except APIError as e:
            last_error = e
            continue
        except Exception as e:
            last_error = e
            continue

    if last_error:
        raise RuntimeError(
            f"Ð’ÑÐµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ ({', '.join(MODELS)}) Ð½Ðµ ÑÐ¼Ð¾Ð³Ð»Ð¸ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ Ð·Ð°Ð¿Ñ€Ð¾Ñ. "
            f"ÐŸÐ¾ÑÐ»ÐµÐ´Ð½ÑÑ Ð¾ÑˆÐ¸Ð±ÐºÐ°: {last_error}"
        ) from last_error

    raise RuntimeError("Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¿ÑƒÑÑ‚")


# ÐœÐ¾Ð´ÑƒÐ»ÑŒ 1: Ð¤Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€ - Ð¿Ñ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÑ‚ Ñ‚ÐµÐºÑÑ‚ Ð·Ð°Ð´Ð°Ñ‡Ð¸ Ð² Ñ„Ð¾Ñ€Ð¼ÑƒÐ»Ñ‹ Ð»Ð¾Ð³Ð¸ÐºÐ¸ Ð¿Ñ€ÐµÐ´Ð¸ÐºÐ°Ñ‚Ð¾Ð²
async def module1_formalize(user_text: str, user_id: str | None = None) -> str:
    system_prompt = _get_prompt("formalizer")
    return await _call_llm(system_prompt, user_text, user_id)


# ÐœÐ¾Ð´ÑƒÐ»ÑŒ 2: Ð”Ð²Ð¸Ð¶Ð¾Ðº Ñ€ÐµÐ·Ð¾Ð»ÑŽÑ†Ð¸Ð¹ - Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼ Ñ€ÐµÐ·Ð¾Ð»ÑŽÑ†Ð¸Ð¹ Ð´Ð»Ñ Ð¿Ð¾Ð¸ÑÐºÐ° Ð¿Ñ€Ð¾Ñ‚Ð¸Ð²Ð¾Ñ€ÐµÑ‡Ð¸Ñ
async def module2_resolve(formulas_str: str) -> tuple[bool, list[str]]:
    formulas = [f.strip() for f in formulas_str.split(',') if f.strip()]
    return resolution_proof(formulas)


# ÐœÐ¾Ð´ÑƒÐ»ÑŒ 3: ÐžÐ±ÑŠÑÑÐ½ÑÑ‚Ð¾Ñ€ - Ð¿Ñ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÑ‚ Ñ„Ð¾Ñ€Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð»Ð¾Ð³ Ð´Ð¾ÐºÐ°Ð·Ð°Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ð° Ð² Ð¿Ð¾Ð½ÑÑ‚Ð½Ð¾Ðµ Ð¾Ð±ÑŠÑÑÐ½ÐµÐ½Ð¸Ðµ
async def module3_explain(proof_log: list[str], user_id: str | None = None) -> str:
    system_prompt = _get_prompt("explainer")
    log_text = "\n".join(proof_log)
    return await _call_llm(system_prompt, log_text, user_id)


# ÐŸÐ¾Ð»Ð½Ñ‹Ð¹ Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½: ÐœÐ¾Ð´ÑƒÐ»ÑŒ 1 â†’ ÐœÐ¾Ð´ÑƒÐ»ÑŒ 2 â†’ ÐœÐ¾Ð´ÑƒÐ»ÑŒ 3
async def full_pipeline(
    user_text: str, 
    user_id: str | None = None,
    progress_callback: Callable[[str], Awaitable[Any]] | None = None
) -> dict[str, str | list[str] | bool]:
    if progress_callback:
        await progress_callback("ðŸ”· ÐœÐ¾Ð´ÑƒÐ»ÑŒ 1: Ð¤Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð·Ð°Ð´Ð°Ñ‡Ð¸...")
    formalized = await module1_formalize(user_text, user_id)
    
    # ÐœÐ¾Ð´ÑƒÐ»ÑŒ 2: Ð ÐµÐ·Ð¾Ð»ÑŽÑ†Ð¸Ð¸
    if progress_callback:
        await progress_callback("ðŸ”· ÐœÐ¾Ð´ÑƒÐ»ÑŒ 2: Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ Ð´Ð¾ÐºÐ°Ð·Ð°Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ð°...")
    proof_found, proof_log = await module2_resolve(formalized)
    
    # ÐœÐ¾Ð´ÑƒÐ»ÑŒ 3: ÐžÐ±ÑŠÑÑÐ½ÐµÐ½Ð¸Ðµ
    if progress_callback:
        await progress_callback("ðŸ”· ÐœÐ¾Ð´ÑƒÐ»ÑŒ 3: Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¾Ð±ÑŠÑÑÐ½ÐµÐ½Ð¸Ñ...")
    explanation = await module3_explain(proof_log, user_id)
    
    return {
        'formalized': formalized,
        'proof_found': proof_found,
        'proof_log': proof_log,
        'explanation': explanation
    }


# Legacy Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾Ð¹ ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼Ð¾ÑÑ‚Ð¸ (Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ð¿Ð¾Ð»Ð½Ñ‹Ð¹ Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½)
async def generate_reply(user_text: str, user_id: str | None = None) -> str:
    result = await full_pipeline(user_text, user_id)
    return result['explanation']


